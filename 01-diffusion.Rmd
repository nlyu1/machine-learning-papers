# Flow Matching and Diffusion {#flowDiffusion}

This section explores flow matching and diffusion models based on the [MIT lecture series](https://diffusion.csail.mit.edu/). Also see lecture notes [@holderrieth2025introduction] and [materials](#https://diffusion.csail.mit.edu/#lectures). More advanced, supplemental proofs are referenced from [@li2025unified]. 

## Modeling {#prelim -}

- We presume an underlying, generally unknown, data distribution $\Pdata$ supported on $\R^d$.
- Dataset consists of finite samples $\{z_1, \dots, z_N\}\sim \Pdata$, effectively yielding an empirical distribution $\hat \Pdata$. 
- Unconditional generation consists of (1) estimating $\Pdata$ from $\hat \Pdata$, then drawing samples from the estimate. 
- Assuming a joint distribution with some conditioning variable $y\sim p_y$, conditional distribution consists of sampling from the conditional distribution $p(\cdot \mid y)$. 

In flow models, we model the data via a **mapping** $u:\R^d\to \R^d$ between standard Gaussian initial distribution  $\Pinit=\mca N(0, I_d)$ and the data distribution. This mapping is modeled as a <span style="color:blue">flow whose time-derivative is parameterized by a deep model</span>. Given a vector field 
\[ 
    (X_t, t)\mapsto u_t(X_t) \quad \text{of type}\quad u_{(\cdot)}(\cdot): \R^d\times \R \to \R^d
\] 
Treating the vector field $u_t$ as an integrable operator, the flow $\Phi_t: \R^d\to \R^d$ is obtained by integrating the vector field from $0$ to time $t$: 
\[ 
    \Phi_t(x_0) = \exp \left[\int_0^t u_\tau\, d\tau\right]x_0 \, \iff \pd t \Phi_t(x_0) = u_t\left[\Phi_t(x_0)\right] 
\] 
Such model above have deterministic dynamics, i.e. once $x_0\sim \Pinit$ is fixed, the sample is fixed. We may instead consider stochastic dynamics: 
\[ 
    dX_t = u_t(X_t)\, dt + \sigma_t\, dW_t \iff \Phi_t(x_0) = \exp \left[
       \int_0^t u_\tau \, d\tau + \int_0^t \sigma_\tau \, dW_\tau 
    \right] x_0 
\] 
where $W_t$ is the standard Brownian motion. This concludes our specification of generation procedure as a function of model parameters $\theta$ and $\sigma_t$. To generate a new data sample from our model: 

1. Sample $\epsilon\sim \Pinit$. 
2. Compute $x_1 = \Phi_1(x_0=\epsilon) = \exp \left[
       \int_0^1 u_\tau \, d\tau + \int_0^1 \sigma^t_\tau \, dW_\tau 
    \right] \epsilon$ using SDE approximation methods. 

## Training procedure {#trainProcedure -}

The next order of business is to train $\theta$ so that $x_1=\Phi_1(\Pinit)=\approx \Pdata$. The main idea is to integrate tractable conditional solutions to obtain the full, marginalized solution. 

:::{.definition #condGaussianPath name="conditional Gaussian probability path"}
Consider a single point $z\in \R^d$, the following **Gaussian probability path** continuously collapses $\Pinit$ to $\delta_z$: 
\[ 
    p_t(\cdot\mid z) = \mca N(\alpha_t z, \beta_t^2 I_d)
\] 
where $\alpha_t, \beta_t$ are **noise schedulers** satisfying $\alpha_0=\beta_1=0$ and $\alpha_1=\beta_0=1$. 
:::

Given a (continuous) probability path $p_{(\cdot)}(\cdot): \R\to \Delta(\R^d)$, we can relate it to a vector field $u_{(\cdot)}:\R\times \R^d\to \R^d$ which realizes it as follows: 

:::{.theorem #contEq name="continuity equation"}
Under regularity conditions, the time-dependent probability path $\{p_t\}$ is generated by the vector field $\{u_t\}$ iff 
\[ 
    \exp \left[
        \int_0^\tau u_\tau \, d\tau 
    \right](\epsilon \sim p_0) \sim p_t \iff \forall x, \tau: \pd t \big|_\tau\, p_t = -\nabla \cdot (u_\tau \rho_\tau) 
\]
Note again that $\exp \left[\int_0^\tau u_\tau \, d\tau \right](\epsilon \sim p_0)$ denotes the variable which satisfies $\pd t x_t = u_t(x_t)$ and $x_0 = \epsilon\sim p_0$. 
:::
<details>
<summary>Use test function and reverse product rule, divergence theorem. The main idea is to write $\pd t \Exp_{p_t}[f]$ as $\Exp_{\pd t p_t}[f]$ as well as a $f$-integral. Expand the $f$-integral using chain rule and product rule, and apply divergence theorem to rid of one term</summary>
Consider arbitrary, time-invariant smooth test function $f$. First convince ourselves that we have 
\[ 
    \int f(x_t) p_0(x_0)\, dx_0 = \int f(x_t) p_t(x_t)\, dx_t 
\] 
To see this rigorously, let $x_t = \Phi_t(x_0)$, then by standard chain rule we have $dx_t = \det (D\Phi_t)\, dx_0$. On the other hand, by pushforward measure we have $p_t(\Phi_t(x_0)) = p_0(x_0) \det^{-1} (D\Phi_t)$. Proceeding: 
\begin{align}
    \pd t \Exp_{p_t} [f] 
    &= \pd t \int f(x_t) p_0(x_0)\, dx_0 = \pd t \int f\left[\exp \left(
        \int_0^t u_t\, d\tau 
    \right) x_0 \right] p_0(x_0)\, dx_0 \\ 
    &= \int \left[\nabla \big|_{x_t} f \right]\cdot u_t(x_t) \left[p_0(x_0)\, dx_0\right] 
    = \int \left[\nabla \big|_{x_t} f \right]\cdot \left[u_t(x_t) p_t(x_t)\right]\, dx_t \\ 
    &= \int \nabla \cdot (f u_t p_t)\big|_{x_t}  - f(x_t) \nabla\cdot (u_tp_t)\big|_{x_t} \, dx_t 
    = -\int f(x)\, \nabla \cdot (u_t p_t)\, dx 
\end{align} 
The first part vanishes since $p_t$ vanishes on far boundaries. 
On the other hand, we also have 
\begin{align}
    \pd t \Exp_{p_t} [f] 
    &= \pd t \int f(x) p_t(x)\, dx = \int f(x)\pd t p_t(x)\, dx 
\end{align}
Combining, we obtain $\pd t p_t = -\nabla \cdot (u_t p_t)$, as desired. 
</details>

Salient points: 

1. Given $u_t$, we can use it to compute $\pd t p_t$; given $p_0$, this gives us the rest of $p_t$. 
2. Given $\{p_t\}$, we can obtain a (non-unique) vector field which generates it by computing $\pd t p_t$ and fitting it to the divergence. This is always possible under regularity conditions because vector fields have more degrees of freedom. 

:::{.theorem #fokkerPlanck name="Fokker-Planck"}
Under regularity conditions, the time-dependent probability path $\{p_t\}$ is generated by the vector field $\{u_t\}$ and volatility process $\sigma_t$ or, mathematically as below: 
\[ 
    \exp \left[
        \int_0^t u_\tau \, d\tau + \int_0^t \sigma_\tau dW_\tau 
    \right](\epsilon \sim p_0) \sim p_t
\]
if and only if the following generalized divergence theorem is satisfied: 
\[ 
    \forall x, \tau: \pd t \big|_\tau\, p_t = -\nabla \cdot (u_\tau \rho_\tau) + \df{\sigma_\tau^2}{2} \nabla^2 p_\tau, \quad \nabla^2 p \equiv \nabla \cdot (\nabla p) = \tr(H_p) 
\] 
Again, note that $u_t$ is a vector field (differentiation operator) thus acts on scalar fields (or random variables) like $p_t$ by application (so we need to apply chain rule), while $\sigma_t$ is simply a scalar, thus acting by multiplication. 
:::
<details>
<summary>Use test function and reverse product rule, divergence theorem</summary>
Fixing $p_0$, let $x_t=\Phi_t(x_0\sim \p_0)$ denote the _random variable_ result from applying flow; note that $p_0:\R^d\to \R$ is a distribution, while $X_0: \Omega\to \R^d$ is a random variable. To the first order, we obtain 
\[ 
    x_{t+h} \approx x_t + h\, u_t(x_t) + \sigma_t (W_{t+h} - W_t)
\] 
Here $u_t(x_t)$ is the tangent random variable denoting the application of $u_t$ to the crystallization of $x_t$. Given a scalar function $f$, we obtain 
\begin{aligned}
    f(x_{t+h}) - f(x_t) 
    &\approx (\nabla \big|_{x_t} f) \cdot \left[
        h\, u_t(x_t) + \sigma_t(W_{t+h} - W_t)\right] \\ 
    &\quad + \df 1 2 \left[
        h\, u_t(x_t) + \sigma_t(W_{t+h} - W_t)\right]^T \left(H_f \big|_{x_t}\right) \left[
        h\, u_t(x_t) + \sigma_t(W_{t+h} - W_t)\right] \\ 
    &= (\nabla f) \cdot \left[
        h\, u_t(x_t) + \sigma_t(W_{t+h} - W_t)\right] 
        + \df {h^2} 2 u_t(x_t)^T \, H_f \, u_t(x_t) \\ 
        &\quad + h \sigma_t \, u_t(x_t) H_f (W_{t+h} - W_t) 
        + \df {\sigma_t^2}{2} (W_{t+h} - W_t)^T H_f  (W_{t+h} - W_t)
\end{aligned}
Taking expectation, note that $\Exp[W_{t+h} - W_t]=0$ since $W_{t+h} - W_t\sim \mca N(0, \sigma^2=h)$. Also note that $\Exp[h^t Ah]=\Exp \tr(A hh^T)=\tr(A)$ for $h\sim \mca N(0, I_d)$, then 
\begin{aligned}
    \Exp[f(x_{t+h}) - f(x_t)]
    &=  \Exp \left[
    (\nabla f) \cdot u_t(x_t)\, h + \df{\sigma_t^2}{2} \tr(H_f) \, h + O(h^2)\right] \\ 
    \pd t \Exp[f(x_t)] &= \Exp\left[(\nabla f)\cdot u_t(x_t) + \df{\sigma_t^2}{2} \tr(H_f)\right]
\end{aligned}
Great! Next up, expand the integral and apply reverse product rule piece by piece. 
The first term is familiar from continuity equation \@ref(thm:contEq): 
\begin{aligned}
    \Exp[(\nabla f) \cdot u_t(x_t)]
    &= \int p_t(x) (\nabla_f) \cdot u_t(x)\, dx 
    = -\int f(x) \nabla \cdot (p_t u_t)\, dx 
\end{aligned}
By applying reverse product rule twice, the second term is seen to be 
\[ 
    \Exp[(\nabla f) \cdot u_t(x_t)]
    = \df{\sigma_t^2}{2} \int (\Delta \big|_x f) p_t(x)\, dx 
    = \df{\sigma_t^2}{2} \int f(x) \Delta p_t\, dx 
\] 
Combining, we have proved that for any regular scalar function $f$, we have 
\begin{aligned}
    \pd t \Exp[f(x_t)] = \int f(x) \left(\nabla \cdot (p_tu_t) + \df{\sigma^2}{2} \Delta p_t\right)\, dx = \int f(x) \pd t p_t(x)\, dx 
\end{aligned}
This shows that $\pd t p_t=\dots$ almost surely; the necessary direction follows from uniqueness of SDEs. 
</details>

### Conditional to marginal {-}

The following theorem is the main tool for converting conditional models into marginal ones. 

:::{.theorem #margTrick name="marginalization trick"}
Fixing $\Pdata$ and conditional probability paths $p_t(\cdot\mid z)$ interpolating between $\Pinit$ and $\delta_z$ (e.g. definition \@ref(def:condGaussianPath)). For each $z\in \R^d$ let $u^*_t(\cdot\mid z):\R^d\to \R^d$ denote a vector field which realizes the conditional probability path, i.e. 
\[ 
    \exp \left[
        \int_0^t
            u^*_\tau(\cdot\mid z)\, d\tau 
        \, d\tau 
    \right]\, (\epsilon \sim \Pinit) \sim p_t(\cdot \mid z) 
\] 
Then the marginal vector field defined as follows 
\[ 
    u_t(x) \equiv \int u_t^*(x\mid z) p_t(z\mid x)\, dz, \quad p_t(z\mid x) = \df{p_t(x\mid z) \Pdata(z)}{p_t(x)}
\] 
realizes the marginal probability distribution 
\[ 
    \exp \left[
        \int_0^t
            u^*_\tau\, d\tau 
    \right]\, (\epsilon \sim \Pinit) \sim p_t = \int p_t(\cdot\mid z)\, dz 
\] 
:::