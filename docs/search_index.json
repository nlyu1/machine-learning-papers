[["index.html", "Machine Learning Theory &amp; Papers Preface", " Machine Learning Theory &amp; Papers Nicholas Lyu 2025-08-15 Preface Continually updated notes for cool machine learning theory and papers. "],["flowDiffusion.html", "1 Flow Matching and Diffusion Modeling Training procedure", " 1 Flow Matching and Diffusion This section explores flow matching and diffusion models based on the MIT lecture series. Also see lecture notes (Holderrieth and Erives 2025) and materials. More advanced, supplemental proofs are referenced from (Li 2025). Modeling We presume an underlying, generally unknown, data distribution \\(p_{\\mathrm{data}}\\) supported on \\(\\mathbb R^d\\). Dataset consists of finite samples \\(\\{z_1, \\dots, z_N\\}\\sim p_{\\mathrm{data}}\\), effectively yielding an empirical distribution \\(\\hat p_{\\mathrm{data}}\\). Unconditional generation consists of (1) estimating \\(p_{\\mathrm{data}}\\) from \\(\\hat p_{\\mathrm{data}}\\), then drawing samples from the estimate. Assuming a joint distribution with some conditioning variable \\(y\\sim p_y\\), conditional distribution consists of sampling from the conditional distribution \\(p(\\cdot \\mid y)\\). In flow models, we model the data via a mapping \\(u:\\mathbb R^d\\to \\mathbb R^d\\) between standard Gaussian initial distribution \\(p_{\\mathrm{init}}=\\mathcal N(0, I_d)\\) and the data distribution. This mapping is modeled as a flow whose time-derivative is parameterized by a deep model. Given a vector field \\[ (X_t, t)\\mapsto u_t(X_t) \\quad \\text{of type}\\quad u_{(\\cdot)}(\\cdot): \\mathbb R^d\\times \\mathbb R\\to \\mathbb R^d \\] Treating the vector field \\(u_t\\) as an integrable operator, the flow \\(\\Phi_t: \\mathbb R^d\\to \\mathbb R^d\\) is obtained by integrating the vector field from \\(0\\) to time \\(t\\): \\[ \\Phi_t(x_0) = \\exp \\left[\\int_0^t u_\\tau\\, d\\tau\\right]x_0 \\, \\iff \\partial_{t} \\Phi_t(x_0) = u_t\\left[\\Phi_t(x_0)\\right] \\] Such model above have deterministic dynamics, i.e.Â once \\(x_0\\sim p_{\\mathrm{init}}\\) is fixed, the sample is fixed. We may instead consider stochastic dynamics: \\[ dX_t = u_t(X_t)\\, dt + \\sigma_t\\, dW_t \\iff \\Phi_t(x_0) = \\exp \\left[ \\int_0^t u_\\tau \\, d\\tau + \\int_0^t \\sigma_\\tau \\, dW_\\tau \\right] x_0 \\] where \\(W_t\\) is the standard Brownian motion. This concludes our specification of generation procedure as a function of model parameters \\(\\theta\\) and \\(\\sigma_t\\). To generate a new data sample from our model: Sample \\(\\epsilon\\sim p_{\\mathrm{init}}\\). Compute \\(x_1 = \\Phi_1(x_0=\\epsilon) = \\exp \\left[ \\int_0^1 u_\\tau \\, d\\tau + \\int_0^1 \\sigma^t_\\tau \\, dW_\\tau \\right] \\epsilon\\) using SDE approximation methods. Training procedure The next order of business is to train \\(\\theta\\) so that \\(x_1=\\Phi_1(p_{\\mathrm{init}})=\\approx p_{\\mathrm{data}}\\). The main idea is to integrate tractable conditional solutions to obtain the full, marginalized solution. Definition 1.1 (conditional Gaussian probability path) Consider a single point \\(z\\in \\mathbb R^d\\), the following Gaussian probability path continuously collapses \\(p_{\\mathrm{init}}\\) to \\(\\delta_z\\): \\[ p_t(\\cdot\\mid z) = \\mathcal N(\\alpha_t z, \\beta_t^2 I_d) \\] where \\(\\alpha_t, \\beta_t\\) are noise schedulers satisfying \\(\\alpha_0=\\beta_1=0\\) and \\(\\alpha_1=\\beta_0=1\\). Given a (continuous) probability path \\(p_{(\\cdot)}(\\cdot): \\mathbb R\\to \\Delta(\\mathbb R^d)\\), we can relate it to a vector field \\(u_{(\\cdot)}:\\mathbb R\\times \\mathbb R^d\\to \\mathbb R^d\\) which realizes it as follows: Theorem 1.1 (continuity equation) Under regularity conditions, the time-dependent probability path \\(\\{p_t\\}\\) is generated by the vector field \\(\\{u_t\\}\\) iff \\[ \\exp \\left[ \\int_0^\\tau u_\\tau \\, d\\tau \\right](\\epsilon \\sim p_0) \\sim p_t \\iff \\forall x, \\tau: \\partial_{t} \\big|_\\tau\\, p_t = -\\nabla \\cdot (u_\\tau \\rho_\\tau) \\] Note again that \\(\\exp \\left[\\int_0^\\tau u_\\tau \\, d\\tau \\right](\\epsilon \\sim p_0)\\) denotes the variable which satisfies \\(\\partial_{t} x_t = u_t(x_t)\\) and \\(x_0 = \\epsilon\\sim p_0\\). Use test function and reverse product rule, divergence theorem. The main idea is to write \\(\\partial_{t} \\mathbb{E}_{p_t}[f]\\) as \\(\\mathbb{E}_{\\partial_{t} p_t}[f]\\) as well as a \\(f\\)-integral. Expand the \\(f\\)-integral using chain rule and product rule, and apply divergence theorem to rid of one term Consider arbitrary, time-invariant smooth test function \\(f\\). First convince ourselves that we have \\[ \\int f(x_t) p_0(x_0)\\, dx_0 = \\int f(x_t) p_t(x_t)\\, dx_t \\] To see this rigorously, let \\(x_t = \\Phi_t(x_0)\\), then by standard chain rule we have \\(dx_t = \\det (D\\Phi_t)\\, dx_0\\). On the other hand, by pushforward measure we have \\(p_t(\\Phi_t(x_0)) = p_0(x_0) \\det^{-1} (D\\Phi_t)\\). Proceeding: \\[\\begin{align} \\partial_{t} \\mathbb{E}_{p_t} [f] &amp;= \\partial_{t} \\int f(x_t) p_0(x_0)\\, dx_0 = \\partial_{t} \\int f\\left[\\exp \\left( \\int_0^t u_t\\, d\\tau \\right) x_0 \\right] p_0(x_0)\\, dx_0 \\\\ &amp;= \\int \\left[\\nabla \\big|_{x_t} f \\right]\\cdot u_t(x_t) \\left[p_0(x_0)\\, dx_0\\right] = \\int \\left[\\nabla \\big|_{x_t} f \\right]\\cdot \\left[u_t(x_t) p_t(x_t)\\right]\\, dx_t \\\\ &amp;= \\int \\nabla \\cdot (f u_t p_t)\\big|_{x_t} - f(x_t) \\nabla\\cdot (u_tp_t)\\big|_{x_t} \\, dx_t = -\\int f(x)\\, \\nabla \\cdot (u_t p_t)\\, dx \\end{align}\\] The first part vanishes since \\(p_t\\) vanishes on far boundaries. On the other hand, we also have \\[\\begin{align} \\partial_{t} \\mathbb{E}_{p_t} [f] &amp;= \\partial_{t} \\int f(x) p_t(x)\\, dx = \\int f(x)\\partial_{t} p_t(x)\\, dx \\end{align}\\] Combining, we obtain \\(\\partial_{t} p_t = -\\nabla \\cdot (u_t p_t)\\), as desired. Salient points: Given \\(u_t\\), we can use it to compute \\(\\partial_{t} p_t\\); given \\(p_0\\), this gives us the rest of \\(p_t\\). Given \\(\\{p_t\\}\\), we can obtain a (non-unique) vector field which generates it by computing \\(\\partial_{t} p_t\\) and fitting it to the divergence. This is always possible under regularity conditions because vector fields have more degrees of freedom. Theorem 1.2 (Fokker-Planck) Under regularity conditions, the time-dependent probability path \\(\\{p_t\\}\\) is generated by the vector field \\(\\{u_t\\}\\) and volatility process \\(\\sigma_t\\) or, mathematically as below: \\[ \\exp \\left[ \\int_0^t u_\\tau \\, d\\tau + \\int_0^t \\sigma_\\tau dW_\\tau \\right](\\epsilon \\sim p_0) \\sim p_t \\] if and only if the following generalized divergence theorem is satisfied: \\[ \\forall x, \\tau: \\partial_{t} \\big|_\\tau\\, p_t = -\\nabla \\cdot (u_\\tau \\rho_\\tau) + \\dfrac{\\sigma_\\tau^2}{2} \\nabla^2 p_\\tau, \\quad \\nabla^2 p \\equiv \\nabla \\cdot (\\nabla p) = \\mathrm{tr}(H_p) \\] Again, note that \\(u_t\\) is a vector field (differentiation operator) thus acts on scalar fields (or random variables) like \\(p_t\\) by application (so we need to apply chain rule), while \\(\\sigma_t\\) is simply a scalar, thus acting by multiplication. Use test function and reverse product rule, divergence theorem Fixing \\(p_0\\), let \\(x_t=\\Phi_t(x_0\\sim \\p_0)\\) denote the random variable result from applying flow; note that \\(p_0:\\mathbb R^d\\to \\mathbb R\\) is a distribution, while \\(X_0: \\Omega\\to \\mathbb R^d\\) is a random variable. To the first order, we obtain \\[ x_{t+h} \\approx x_t + h\\, u_t(x_t) + \\sigma_t (W_{t+h} - W_t) \\] Here \\(u_t(x_t)\\) is the tangent random variable denoting the application of \\(u_t\\) to the crystallization of \\(x_t\\). Given a scalar function \\(f\\), we obtain \\[\\begin{aligned} f(x_{t+h}) - f(x_t) &amp;\\approx (\\nabla \\big|_{x_t} f) \\cdot \\left[ h\\, u_t(x_t) + \\sigma_t(W_{t+h} - W_t)\\right] \\\\ &amp;\\quad + \\dfrac 1 2 \\left[ h\\, u_t(x_t) + \\sigma_t(W_{t+h} - W_t)\\right]^T \\left(H_f \\big|_{x_t}\\right) \\left[ h\\, u_t(x_t) + \\sigma_t(W_{t+h} - W_t)\\right] \\\\ &amp;= (\\nabla f) \\cdot \\left[ h\\, u_t(x_t) + \\sigma_t(W_{t+h} - W_t)\\right] + \\dfrac{h^2} 2 u_t(x_t)^T \\, H_f \\, u_t(x_t) \\\\ &amp;\\quad + h \\sigma_t \\, u_t(x_t) H_f (W_{t+h} - W_t) + \\dfrac{\\sigma_t^2}{2} (W_{t+h} - W_t)^T H_f (W_{t+h} - W_t) \\end{aligned}\\] Taking expectation, note that \\(\\mathbb{E}[W_{t+h} - W_t]=0\\) since \\(W_{t+h} - W_t\\sim \\mathcal N(0, \\sigma^2=h)\\). Also note that \\(\\mathbb{E}[h^t Ah]=\\mathbb{E}\\mathrm{tr}(A hh^T)=\\mathrm{tr}(A)\\) for \\(h\\sim \\mathcal N(0, I_d)\\), then \\[\\begin{aligned} \\mathbb{E}[f(x_{t+h}) - f(x_t)] &amp;= \\mathbb{E}\\left[ (\\nabla f) \\cdot u_t(x_t)\\, h + \\dfrac{\\sigma_t^2}{2} \\mathrm{tr}(H_f) \\, h + O(h^2)\\right] \\\\ \\partial_{t} \\mathbb{E}[f(x_t)] &amp;= \\mathbb{E}\\left[(\\nabla f)\\cdot u_t(x_t) + \\dfrac{\\sigma_t^2}{2} \\mathrm{tr}(H_f)\\right] \\end{aligned}\\] Great! Next up, expand the integral and apply reverse product rule piece by piece. The first term is familiar from continuity equation 1.1: \\[\\begin{aligned} \\mathbb{E}[(\\nabla f) \\cdot u_t(x_t)] &amp;= \\int p_t(x) (\\nabla_f) \\cdot u_t(x)\\, dx = -\\int f(x) \\nabla \\cdot (p_t u_t)\\, dx \\end{aligned}\\] By applying reverse product rule twice, the second term is seen to be \\[ \\mathbb{E}[(\\nabla f) \\cdot u_t(x_t)] = \\dfrac{\\sigma_t^2}{2} \\int (\\Delta \\big|_x f) p_t(x)\\, dx = \\dfrac{\\sigma_t^2}{2} \\int f(x) \\Delta p_t\\, dx \\] Combining, we have proved that for any regular scalar function \\(f\\), we have \\[\\begin{aligned} \\partial_{t} \\mathbb{E}[f(x_t)] = \\int f(x) \\left(\\nabla \\cdot (p_tu_t) + \\dfrac{\\sigma^2}{2} \\Delta p_t\\right)\\, dx = \\int f(x) \\partial_{t} p_t(x)\\, dx \\end{aligned}\\] This shows that \\(\\partial_{t} p_t=\\dots\\) almost surely; the necessary direction follows from uniqueness of SDEs. Conditional to marginal The following theorem is the main tool for converting conditional models into marginal ones. Theorem 1.3 (marginalization trick) Fixing \\(p_{\\mathrm{data}}\\) and conditional probability paths \\(p_t(\\cdot\\mid z)\\) interpolating between \\(p_{\\mathrm{init}}\\) and \\(\\delta_z\\) (e.g.Â definition 1.1). For each \\(z\\in \\mathbb R^d\\) let \\(u^*_t(\\cdot\\mid z):\\mathbb R^d\\to \\mathbb R^d\\) denote a vector field which realizes the conditional probability path, i.e.Â  \\[ \\exp \\left[ \\int_0^t u^*_\\tau(\\cdot\\mid z)\\, d\\tau \\, d\\tau \\right]\\, (\\epsilon \\sim p_{\\mathrm{init}}) \\sim p_t(\\cdot \\mid z) \\] Then the marginal vector field defined as follows \\[ u_t(x) \\equiv \\int u_t^*(x\\mid z) p_t(z\\mid x)\\, dz, \\quad p_t(z\\mid x) = \\dfrac{p_t(x\\mid z) p_{\\mathrm{data}}(z)}{p_t(x)} \\] realizes the marginal probability distribution \\[ \\exp \\left[ \\int_0^t u^*_\\tau\\, d\\tau \\right]\\, (\\epsilon \\sim p_{\\mathrm{init}}) \\sim p_t = \\int p_t(\\cdot\\mid z)\\, dz \\] Bibliography "],["bibliography.html", "Bibliography", " Bibliography "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
