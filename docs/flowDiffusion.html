<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Flow Matching and Diffusion | Machine Learning Theory &amp; Papers</title>
  <meta name="description" content="1 Flow Matching and Diffusion | Machine Learning Theory &amp; Papers" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Flow Matching and Diffusion | Machine Learning Theory &amp; Papers" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Flow Matching and Diffusion | Machine Learning Theory &amp; Papers" />
  
  
  

<meta name="author" content="Nicholas Lyu" />


<meta name="date" content="2025-08-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="bibliography.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="flowDiffusion.html"><a href="flowDiffusion.html"><i class="fa fa-check"></i><b>1</b> Flow Matching and Diffusion</a>
<ul>
<li class="chapter" data-level="" data-path="flowDiffusion.html"><a href="flowDiffusion.html#prelim"><i class="fa fa-check"></i>Modeling</a></li>
<li class="chapter" data-level="" data-path="flowDiffusion.html"><a href="flowDiffusion.html#vecFieldsAndDistributions"><i class="fa fa-check"></i>Vector fields and distributions</a></li>
<li class="chapter" data-level="" data-path="flowDiffusion.html"><a href="flowDiffusion.html#marginalization"><i class="fa fa-check"></i>Marginalization</a></li>
<li class="chapter" data-level="" data-path="flowDiffusion.html"><a href="flowDiffusion.html#optimization"><i class="fa fa-check"></i>Optimization</a></li>
<li class="chapter" data-level="" data-path="flowDiffusion.html"><a href="flowDiffusion.html#gaussian"><i class="fa fa-check"></i>Gaussian example</a></li>
<li class="chapter" data-level="" data-path="flowDiffusion.html"><a href="flowDiffusion.html#guidance"><i class="fa fa-check"></i>Guidance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Theory &amp; Papers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="flowDiffusion" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> Flow Matching and Diffusion<a href="flowDiffusion.html#flowDiffusion" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This section explores flow matching and diffusion models based on the <a href="https://diffusion.csail.mit.edu/">MIT lecture series</a>. Also see lecture notes <span class="citation">(<a href="#ref-holderrieth2025introduction">Holderrieth and Erives 2025</a>)</span> and <a href="https://diffusion.csail.mit.edu/#lectures">materials</a>. More advanced, supplemental proofs are referenced from <span class="citation">(<a href="#ref-li2025unified">Li 2025</a>)</span>.</p>
<ol style="list-style-type: decimal">
<li>Flow models parameterize the <strong>vector field</strong> (in the same space as data, for Euclidean data) which induces a specified probability path. Diffusion models generalize by adding Brownian noise, PDE <span class="math inline">\(\to\)</span> SDE.</li>
<li>Crucial bridge between vector field and probability path: continuity equation <a href="flowDiffusion.html#thm:contEq">1.1</a> resp. <a href="flowDiffusion.html#thm:fokkerPlanck">1.2</a> Fokker-Planck in SDE case.
<ul>
<li>Given the probabaility velocity vector field <span class="math inline">\(u_t\)</span> and <span class="math inline">\(p_{\mathrm{init}}\)</span>, we can directly compute <span class="math inline">\(\partial_{t} p_t\)</span> via the Fokker-Planck equation.
Conversely, given <span class="math inline">\(p_t\)</span>, we can always construct (non-unique) vector field which matches the probability path.</li>
</ul></li>
<li>Conditioning on a single data point <span class="math inline">\(z\)</span> with dirac delta target <span class="math inline">\(\delta_z\)</span>, the conditional vector field has simple analytic solution. Apply the marginalization trick <a href="flowDiffusion.html#thm:margTrick">1.3</a> to generalize to whole empirical data distribution.
<ul>
<li><u>Very important remark</u>: fixing <span class="math inline">\(p_{\mathrm{init}}\)</span>, the mapping <span class="math inline">\(u_t\mapsto p_t\)</span> <strong>is not linear</strong>. The vector field <span class="math inline">\(u_t\)</span> specifies the <strong>probability velocity</strong>, while mixture linearly mixes the <strong>probability flux</strong> <span class="math inline">\(u_tp_t\)</span>.</li>
</ul></li>
<li>The marginal vector field <a href="flowDiffusion.html#thm:margTrick">1.3</a> and score generally have intractable integrals, but crucially, optimizing MSE to the conditional v. marginal distribution have the same desired behavior (theorem <a href="flowDiffusion.html#thm:opEq">1.4</a>).</li>
<li><span style="blue"> Key leverage via deep learning </span>: having solved the simple conditional case analytically, we’re relying on the model to:
<ul>
<li>Model marginal quantities by optimization equivalence, without resorting to intractable integrals <strong>(very smart theme!!)</strong></li>
<li>Generalize / interpolate the vector field to assign mass to unseen regions of the data manifold.</li>
</ul></li>
<li>Under the <a href="@gaussian">Gaussian probability path</a>, the network essentially learns to predict the noise which is used to corrupt data; conversion between score vs target network targets <a href="flowDiffusion.html#prp:gaussianFormulae">1.3</a>.</li>
<li>Classifier free guidance <a href="flowDiffusion.html#def:cfg">1.3</a> contrastively amplifies the guided v. unguided vector fields. Under Gaussian path, this has the nice interpretation of manually upweighing the score contribution from classification log-likelihood. See detailed training procedure <a href="flowDiffusion.html#def:cfgProtocol">1.4</a>.</li>
</ol>
<div id="prelim" class="section level2 unnumbered hasAnchor">
<h2>Modeling<a href="flowDiffusion.html#prelim" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>We presume an underlying, generally unknown, data distribution <span class="math inline">\(p_{\mathrm{data}}\)</span> supported on <span class="math inline">\(\mathbb R^d\)</span>.</li>
<li>Dataset consists of finite samples <span class="math inline">\(\{z_1, \dots, z_N\}\sim p_{\mathrm{data}}\)</span>, effectively yielding an empirical distribution <span class="math inline">\(\hat p_{\mathrm{data}}\)</span>.</li>
<li>Unconditional generation consists of (1) estimating <span class="math inline">\(p_{\mathrm{data}}\)</span> from <span class="math inline">\(\hat p_{\mathrm{data}}\)</span>, then drawing samples from the estimate.</li>
<li>Assuming a joint distribution with some conditioning variable <span class="math inline">\(y\sim p_y\)</span>, conditional distribution consists of sampling from the conditional distribution <span class="math inline">\(p(\cdot \mid y)\)</span>.</li>
</ul>
<p>In flow models, we model the data via a <strong>mapping</strong> <span class="math inline">\(u:\mathbb R^d\to \mathbb R^d\)</span> between standard Gaussian initial distribution <span class="math inline">\(p_{\mathrm{init}}=\mathcal N(0, I_d)\)</span> and the data distribution. This mapping is modeled as a <span style="color:blue">flow whose time-derivative is parameterized by a deep model</span>. Given a vector field
<span class="math display">\[
    (X_t, t)\mapsto u_t(X_t) \quad \text{of type}\quad u_{(\cdot)}(\cdot): \mathbb R^d\times \mathbb R\to \mathbb R^d
\]</span>
Treating the vector field <span class="math inline">\(u_t\)</span> as an integrable operator, the flow <span class="math inline">\(\Phi_t: \mathbb R^d\to \mathbb R^d\)</span> is obtained by integrating the vector field from <span class="math inline">\(0\)</span> to time <span class="math inline">\(t\)</span>:
<span class="math display">\[
    \Phi_t(x_0) = \exp \left[\int_0^t u_\tau\, d\tau\right]x_0 \, \iff \partial_{t} \Phi_t(x_0) = u_t\left[\Phi_t(x_0)\right]
\]</span>
Such model above have deterministic dynamics, i.e. once <span class="math inline">\(x_0\sim p_{\mathrm{init}}\)</span> is fixed, the sample is fixed. We may instead consider stochastic dynamics:
<span class="math display">\[
    dX_t = u_t(X_t)\, dt + \sigma_t\, dW_t \iff \Phi_t(x_0) = \exp \left[
       \int_0^t u_\tau \, d\tau + \int_0^t \sigma_\tau \, dW_\tau
    \right] x_0
\]</span>
where <span class="math inline">\(W_t\)</span> is the standard Brownian motion. This concludes our specification of generation procedure as a function of model parameters <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\sigma_t\)</span>. To generate a new data sample from our model:</p>
<ol style="list-style-type: decimal">
<li>Sample <span class="math inline">\(\epsilon\sim p_{\mathrm{init}}\)</span>.</li>
<li>Compute <span class="math inline">\(x_1 = \Phi_1(x_0=\epsilon) = \exp \left[  \int_0^1 u_\tau \, d\tau + \int_0^1 \sigma^t_\tau \, dW_\tau  \right] \epsilon\)</span> using SDE approximation methods.</li>
</ol>
</div>
<div id="vecFieldsAndDistributions" class="section level2 unnumbered hasAnchor">
<h2>Vector fields and distributions<a href="flowDiffusion.html#vecFieldsAndDistributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The next order of business is to train <span class="math inline">\(\theta\)</span> so that <span class="math inline">\(x_1=\Phi_1(p_{\mathrm{init}})\approx p_{\mathrm{data}}\)</span>. The main idea is to integrate tractable conditional solutions to obtain the full, marginalized solution.
Given a (continuous) probability path <span class="math inline">\(p_{(\cdot)}(\cdot): \mathbb R\to \Delta(\mathbb R^d)\)</span>, we can relate it to a vector field <span class="math inline">\(u_{(\cdot)}:\mathbb R\times \mathbb R^d\to \mathbb R^d\)</span> which realizes it as follows:</p>
<div class="theorem">
<p><span id="thm:contEq" class="theorem"><strong>Theorem 1.1  (continuity equation) </strong></span>Under regularity conditions, define <span class="math inline">\(x_0\sim p_{\mathrm{init}}\)</span> (note that <span class="math inline">\(x_0\)</span> is a random variable while <span class="math inline">\(p_{\mathrm{init}}\)</span> is a distribution) and <span class="math inline">\(x_t\)</span> as the image of the flow
<span class="math display">\[
    x_t = \exp \left[
        \int_0^\tau u_\tau \, d\tau
    \right]\, x_0\iff \partial_{t} x_t = u_t(x_t)
\]</span>
Let <span class="math inline">\(p_t\)</span> denote the induced distribution of <span class="math inline">\(x_t\)</span>, then <span class="math inline">\(\partial_{t} \big|_\tau\, p_t = -\nabla \cdot (u_\tau \rho_\tau)\)</span>. The converse also holds, i.e. t |<em>, p_t = -(u</em>_)t x_t = u_t(x_t)$.</p>
</div>
<details>
<summary>
Use test function and reverse product rule, divergence theorem. The main idea is to write <span class="math inline">\(\partial_{t} \mathbb{E}_{p_t}[f]\)</span> as <span class="math inline">\(\mathbb{E}_{\partial_{t} p_t}[f]\)</span> as well as a <span class="math inline">\(f\)</span>-integral. Expand the <span class="math inline">\(f\)</span>-integral using chain rule and product rule, and apply divergence theorem to rid of one term
</summary>
Consider arbitrary, time-invariant smooth test function <span class="math inline">\(f\)</span>. First convince ourselves that we have
<span class="math display">\[
    \int f(x_t) p_0(x_0)\, dx_0 = \int f(x_t) p_t(x_t)\, dx_t
\]</span>
To see this rigorously, let <span class="math inline">\(x_t = \Phi_t(x_0)\)</span>, then by standard chain rule we have <span class="math inline">\(dx_t = \det (D\Phi_t)\, dx_0\)</span>. On the other hand, by pushforward measure we have <span class="math inline">\(p_t(\Phi_t(x_0)) = p_0(x_0) \det^{-1} (D\Phi_t)\)</span>. Proceeding:
<span class="math display">\[\begin{align}
    \partial_{t} \mathbb{E}_{p_t} [f]
    &amp;= \partial_{t} \int f(x_t) p_0(x_0)\, dx_0 = \partial_{t} \int f\left[\exp \left(
        \int_0^t u_t\, d\tau
    \right) x_0 \right] p_0(x_0)\, dx_0 \\
    &amp;= \int \left[\nabla \big|_{x_t} f \right]\cdot u_t(x_t) \left[p_0(x_0)\, dx_0\right]
    = \int \left[\nabla \big|_{x_t} f \right]\cdot \left[u_t(x_t) p_t(x_t)\right]\, dx_t \\
    &amp;= \int \nabla \cdot (f u_t p_t)\big|_{x_t}  - f(x_t) \nabla\cdot (u_tp_t)\big|_{x_t} \, dx_t
    = -\int f(x)\, \nabla \cdot (u_t p_t)\, dx
\end{align}\]</span>
The first part vanishes since <span class="math inline">\(p_t\)</span> vanishes on far boundaries.
On the other hand, we also have
<span class="math display">\[\begin{align}
    \partial_{t} \mathbb{E}_{p_t} [f]
    &amp;= \partial_{t} \int f(x) p_t(x)\, dx = \int f(x)\partial_{t} p_t(x)\, dx
\end{align}\]</span>
Combining, we obtain <span class="math inline">\(\partial_{t} p_t = -\nabla \cdot (u_t p_t)\)</span>, as desired.
</details>
<p>Salient points:</p>
<ol style="list-style-type: decimal">
<li>Given <span class="math inline">\(u_t\)</span>, we can use it to compute <span class="math inline">\(\partial_{t} p_t\)</span>; given <span class="math inline">\(p_0\)</span>, this gives us the rest of <span class="math inline">\(p_t\)</span>.</li>
<li>Given <span class="math inline">\(\{p_t\}\)</span>, we can obtain a (non-unique) vector field which generates it by computing <span class="math inline">\(\partial_{t} p_t\)</span> and fitting it to the divergence. This is always possible under regularity conditions because vector fields have more degrees of freedom.</li>
</ol>
<div class="theorem">
<p><span id="thm:fokkerPlanck" class="theorem"><strong>Theorem 1.2  (Fokker-Planck) </strong></span>Under regularity conditions, let <span class="math inline">\(x_0\sim p_{\mathrm{init}}\)</span> and similarly define
<span class="math display">\[
    x_t = \exp \left[
        \int_0^t u_\tau \, d\tau + \int_0^t \sigma_\tau dW_\tau
    \right]\, x_0 \iff dx_t = u_t(x_t)\, dt + \sigma_t\, dW_t
\]</span>
Define <span class="math inline">\(p_t\)</span> such that <span class="math inline">\(x_t\sim p_t\)</span> and define the Lagrangian operator for scalar function <span class="math inline">\(\nabla^2 p \equiv \nabla \cdot (\nabla p) = \mathrm{tr}(H_p)\)</span>. Then
<span class="math display">\[
    \forall x, \tau: \partial_{t} \big|_\tau\, p_t = -\nabla \cdot (u_\tau \rho_\tau) + \dfrac{\sigma_\tau^2}{2} \nabla^2 p_\tau
\]</span>
Again, note that <span class="math inline">\(u_t\)</span> is a vector field (differentiation operator) thus acts on scalar fields (or random variables) like <span class="math inline">\(p_t\)</span> by application (so we need to apply chain rule), while <span class="math inline">\(\sigma_t\)</span> is simply a scalar, thus acting by multiplication.</p>
</div>
<details>
<summary>
Use test function and reverse product rule, divergence theorem
</summary>
To the first order, we obtain
<span class="math display">\[
    x_{t+h} \approx x_t + h\, u_t(x_t) + \sigma_t (W_{t+h} - W_t)
\]</span>
Here <span class="math inline">\(u_t(x_t)\)</span> is the tangent random variable denoting the application of <span class="math inline">\(u_t\)</span> to the crystallization of <span class="math inline">\(x_t\)</span>. Given a scalar function <span class="math inline">\(f\)</span>, we obtain
<span class="math display">\[\begin{aligned}
    f(x_{t+h}) - f(x_t)
    &amp;\approx (\nabla \big|_{x_t} f) \cdot \left[
        h\, u_t(x_t) + \sigma_t(W_{t+h} - W_t)\right] \\
    &amp;\quad + \dfrac 1 2 \left[
        h\, u_t(x_t) + \sigma_t(W_{t+h} - W_t)\right]^T \left(H_f \big|_{x_t}\right) \left[
        h\, u_t(x_t) + \sigma_t(W_{t+h} - W_t)\right] \\
    &amp;= (\nabla f) \cdot \left[
        h\, u_t(x_t) + \sigma_t(W_{t+h} - W_t)\right]
        + \dfrac{h^2} 2 u_t(x_t)^T \, H_f \, u_t(x_t) \\
        &amp;\quad + h \sigma_t \, u_t(x_t) H_f (W_{t+h} - W_t)
        + \dfrac{\sigma_t^2}{2} (W_{t+h} - W_t)^T H_f  (W_{t+h} - W_t)
\end{aligned}\]</span>
Taking expectation, note that <span class="math inline">\(\mathbb{E}[W_{t+h} - W_t]=0\)</span> since <span class="math inline">\(W_{t+h} - W_t\sim \mathcal N(0, \sigma^2=h)\)</span>. Also note that <span class="math inline">\(\mathbb{E}[h^t Ah]=\mathbb{E}\mathrm{tr}(A hh^T)=\mathrm{tr}(A)\)</span> for <span class="math inline">\(h\sim \mathcal N(0, I_d)\)</span>, then
<span class="math display">\[\begin{aligned}
    \mathbb{E}[f(x_{t+h}) - f(x_t)]
    &amp;=  \mathbb{E}\left[
    (\nabla f) \cdot u_t(x_t)\, h + \dfrac{\sigma_t^2}{2} \mathrm{tr}(H_f) \, h + O(h^2)\right] \\
    \partial_{t} \mathbb{E}[f(x_t)] &amp;= \mathbb{E}\left[(\nabla f)\cdot u_t(x_t) + \dfrac{\sigma_t^2}{2} \mathrm{tr}(H_f)\right]
\end{aligned}\]</span>
Great! Next up, expand the integral and apply reverse product rule piece by piece.
The first term is familiar from continuity equation <a href="flowDiffusion.html#thm:contEq">1.1</a>:
<span class="math display">\[\begin{aligned}
    \mathbb{E}[(\nabla f) \cdot u_t(x_t)]
    &amp;= \int p_t(x) (\nabla_f) \cdot u_t(x)\, dx
    = -\int f(x) \nabla \cdot (p_t u_t)\, dx
\end{aligned}\]</span>
By applying reverse product rule twice, the second term is seen to be
<span class="math display">\[
    \mathbb{E}[(\nabla f) \cdot u_t(x_t)]
    = \dfrac{\sigma_t^2}{2} \int (\Delta \big|_x f) p_t(x)\, dx
    = \dfrac{\sigma_t^2}{2} \int f(x) \Delta p_t\, dx
\]</span>
Combining, we have proved that for any regular scalar function <span class="math inline">\(f\)</span>, we have
<span class="math display">\[\begin{aligned}
    \partial_{t} \mathbb{E}[f(x_t)] = \int f(x) \left(\nabla \cdot (p_tu_t) + \dfrac{\sigma^2}{2} \Delta p_t\right)\, dx = \int f(x) \partial_{t} p_t(x)\, dx
\end{aligned}\]</span>
This shows that <span class="math inline">\(\partial_{t} p_t=\dots\)</span> almost surely; the necessary direction follows from uniqueness of SDEs.
</details>
<div class="remark">
<p><span id="unlabeled-div-1" class="remark"><em>Remark</em> (Langevin dynamics). </span>Fixing <span class="math inline">\(p_{\mathrm{init}}\)</span> and set <span class="math inline">\(\partial_{t} p_t=0\)</span> yields that the dynamics
<span class="math display">\[
    dx_t = \dfrac{\sigma_t^2}{2} \nabla \log p(x_t)\, dt + \sigma_t\, dW_t
\]</span>
has stationary distribution <span class="math inline">\(p_{\mathrm{init}}\)</span>. This is called <strong>Langevin dynamics</strong>. In fact, under mild regularity conditions this SDE pulls any initial law <span class="math inline">\(p&#39;\)</span> to the fixed point.</p>
</div>
</div>
<div id="marginalization" class="section level2 unnumbered hasAnchor">
<h2>Marginalization<a href="flowDiffusion.html#marginalization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The following theorem is the main tool for converting conditional models into marginal ones.</p>
<div class="theorem">
<p><span id="thm:margTrick" class="theorem"><strong>Theorem 1.3  (marginalization trick) </strong></span>Fixing <span class="math inline">\(p_{\mathrm{data}}\)</span> and conditional probability paths <span class="math inline">\(p_t(\cdot\mid z)\)</span> interpolating between <span class="math inline">\(p_{\mathrm{init}}\)</span> and <span class="math inline">\(\delta_z\)</span> (e.g. definition <a href="flowDiffusion.html#def:condGaussianPath">1.1</a>). For each <span class="math inline">\(z\in \mathbb R^d\)</span> let <span class="math inline">\(u^*_t(\cdot\mid z):\mathbb R^d\to \mathbb R^d\)</span> denote a vector field which realizes the conditional probability path, i.e. 
<span class="math display">\[
    \exp \left[
        \int_0^t
            u^*_\tau(\cdot\mid z)\, d\tau
        \, d\tau
    \right]\, (\epsilon \sim p_{\mathrm{init}}) \sim p_t(\cdot \mid z)
\]</span>
Then the marginal vector field defined as follows
<span class="math display">\[
    u_t(x) \equiv \int u_t^*(x\mid z) p_t(z\mid x)\, dz, \quad p_t(z\mid x) = \dfrac{p_t(x\mid z) p_{\mathrm{data}}(z)}{p_t(x)}
\]</span>
realizes the marginal probability distribution
<span class="math display">\[
    \exp \left[
        \int_0^t
            u^*_\tau\, d\tau
    \right]\, (\epsilon \sim p_{\mathrm{init}}) \sim p_t = \int p_t(\cdot\mid z)\, dz
\]</span></p>
</div>
Per continuity equation <a href="flowDiffusion.html#thm:contEq">1.1</a>, it suffices to check that the divergence of the proposed vector field matches the time-derivative of the marginal distribution <span class="math inline">\(p_t\)</span>:
<span class="math display">\[\begin{aligned}
    -\nabla \cdot (u_t p_t) \big|_x
    &amp;= -\nabla \cdot \left[
        p_t(x) \int u_t^*(x\mid z) p_t(z\mid x)\, dz
    \right] \\
    &amp;= -\int \nabla \cdot \left[u_t^*(x\mid z) p_t(x\mid z) p_{\mathrm{data}}(z) \right]\, dz \\
    &amp;= -\int p_{\mathrm{data}}(z)\, \nabla \cdot \left[u_t^*(x\mid z) p_t(x\mid z)\right]\, dz \\
    &amp;= \int p_{\mathrm{data}}(z) \partial_{t} p_t(x\mid z)\, dz = \partial_{t} p_t(x)
\end{aligned}\]</span>
<p>Several important points:</p>
<ol style="list-style-type: decimal">
<li>Fixing <span class="math inline">\(t\)</span>, we’re working with the joint distribution over <span class="math inline">\(z\sim p_{\mathrm{data}}\)</span> and <span class="math inline">\(p_t(x_t\mid z)\)</span>. This underpins the simplification <span class="math inline">\(p_t(x) p_t(z\mid x) = p_t(x\mid z) p_{\mathrm{data}}(z)\)</span>.</li>
<li>Divergence is taken w.r.t. the <span class="math inline">\(x\)</span> variable! This is why <span class="math inline">\(p_{\mathrm{data}}(z)\)</span> slides out of divergence as a constant.</li>
<li>Proof sketch: compute the divergence by substituting ansatz for the marginal vector field <span class="math inline">\(u_t\)</span>, apply continuity equation to conditional vector field to convert divergence to time-derivative, then marginalize.</li>
</ol>
<details>
<summary>
<span style="color:blue"> IMPORTANT: why <span class="math inline">\(u_t\neq \int u_t^*(\cdot \mid z) p_{\mathrm{data}}(z)\, dz\)</span></span>
</summary>
Zeroth-order intuition seems to suggest that “since everything is linear”, let’s just substitute <span class="math inline">\(u_t(x) = \int u_t^*(x\mid z) p_{\mathrm{data}}(z)\, dz\)</span>. However, invoking the same machinery yields:
<span class="math display">\[\begin{aligned}
    -\nabla \cdot (u_t p_t) \big|_x
    &amp;= -\nabla \cdot \left[
        p_t(x) \int u_t^*(x\mid z) p_{\mathrm{data}}(z) \, dz
    \right] \\
    &amp;= -\int \nabla \cdot \left[u_t^*(x\mid z) p_t(x\mid z) \dfrac{p_t(x)p_{\mathrm{data}}(z)}{p_t(x\mid z)} \right]\, dz
\end{aligned}\]</span>
Importantly, we don’t obtain the clean refactoring of <span class="math inline">\(p_{\mathrm{data}}(z)\)</span>. The main reason why linear superposition is not correct is because the operator <span class="math inline">\(u_t\mapsto \partial_{t} p_t = -\nabla \cdot (u_tp_t)\)</span> which maps <span class="math inline">\(u_t\)</span> to the distribution it effects <strong>has additional dependence upon <span class="math inline">\(p_t\)</span></strong>, which <strong>depends nontrivially upon <span class="math inline">\(x\)</span></strong>. Switching from conditional to marginal, we have <span class="math inline">\(p_t(x \mid z)\mapsto p_t(x)=\int p_t(x\mid z)p_{\mathrm{data}}(z)\, dz\)</span>; this introduces nontrivial <span class="math inline">\(x\)</span>-dependence inside the divergence operator.
</details>
<p><span style="color:green">
Alternative explanation:
</span> the marginal velocity at a point is the average over all conditional velocities, so <span class="math inline">\(u_t(x) = \mathbb{E}_{z}[u_t(x\mid z) \mid X=x]\)</span>. It’s critical to recognize here that the ensembling and conditioning is done w.r.t. the joint <span class="math inline">\(p_t(x, z)\)</span>.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-2" class="proposition"><strong>Proposition 1.1  (marginalization of score function) </strong></span>Given a distribution <span class="math inline">\(p\)</span>, define its score function <span class="math inline">\(\nabla \log p(x)\)</span>. The marginal score is connected to conditional score by
<span class="math display">\[
    \nabla \log p_t(x_t) = \int p_t(z\mid x_t) \nabla \log p_t(x_t\mid z)\, dz, \quad p_t(z\mid x_t) = \dfrac{p_t(x\mid z)p_{\mathrm{data}}(z)}{p_t(x)}
\]</span></p>
</div>
</div>
<div id="optimization" class="section level2 unnumbered hasAnchor">
<h2>Optimization<a href="flowDiffusion.html#optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
Fixing <span class="math inline">\(p_{\mathrm{data}}\)</span> and <span class="math inline">\(p_t^\mathrm{target}\)</span>, we can mathematically formulate <span class="math inline">\(u_t^\mathrm{target}\)</span> using the continuity equation and marginalization trick. We parameterize <span class="math inline">\(u_t^\theta\)</span> in hopes of approximating <span class="math inline">\(u_t^\mathrm{target}\)</span>. This can be done by minimizing the (marginal) <strong>flow matching loss</strong>
<span class="math display">\[
    \mathcal L_\mathrm{FM}(\theta) = \mathbb{E}_{t\sim \mathrm{Unif}, z\sim p_{\mathrm{data}}, x\sim p_t(\cdot\mid z)} \left\|u_t^\theta(x) - u_t^\mathrm{target}(x)\right\|^2
\]</span>
The only problem being that <span class="math inline">\(u_t^\mathrm{target}\)</span> is an intractable integral (sum for discrete datasets). A much more tractable quantity is the <strong>conditional flow matching loss</strong> which, upon drawing <span class="math inline">\(z\sim p_{\mathrm{data}}\)</span>, only asks the model to regress the conditional vector field for that <span class="math inline">\(z\)</span>:
<span class="math display">\[
    \mathcal L_\mathrm{CFM}(\theta) = \mathbb{E}_{t\sim \mathrm{Unif}, z\sim p_{\mathrm{data}}, x\sim p_t(\cdot\mid z)} \left\|u_t^\theta(x) - u_t^\mathrm{target}(x\mid z)\right\|^2
\]</span>
Similarly define the <strong>score matching</strong> and <strong>conditional score matching</strong> losses:
<span class="math display">\[\begin{aligned}
    \mathcal L_{\mathrm{SM}}(\theta) &amp;= \mathbb{E}_{t\sim \mathrm{Unif}, x\sim p_t} \left\|\nabla \log p_t^\theta(x) - \nabla \log p_t^{\mathrm{target}}(x)\right\|^2 \\
    \mathcal L_{\mathrm{CSM}}(\theta) &amp;= \mathbb{E}_{t\sim \mathrm{Unif}, z\sim p_{\mathrm{data}}, x\sim p_t(\cdot\mid z)} \left\|\nabla \log p_t^\theta(x) - \nabla \log p_t^{\mathrm{target}}(x\mid z)\right\|^2
\end{aligned}\]</span>
<div class="theorem">
<p><span id="thm:opEq" class="theorem"><strong>Theorem 1.4  (optimization equivalence) </strong></span><span class="math inline">\(\nabla_\theta \mathcal L_\mathrm{FM}(\theta) = \nabla_\theta \mathcal L_\mathrm{CFM}(\theta)\)</span>. Similarly, <span class="math inline">\(\nabla_\theta \mathcal L_\mathrm{SM}(\theta) = \nabla_\theta \mathcal L_\mathrm{CSM}(\theta)\)</span>.</p>
</div>
<em>Proof:</em> Expanding the squared norm for both losses, it suffices to show that
<span class="math display">\[
    \mathbb{E}[u_t^\theta(x)^T u_t^\mathrm{target}(x)]
    = \mathbb{E}[u_t^\theta(x)^T u_t^\mathrm{target}(x\mid z)]
\]</span>
Expand integrals and apply the marginalization equation <a href="flowDiffusion.html#thm:margTrick">1.3</a>:
<span class="math display">\[\begin{aligned}
    \mathbb{E}_{t, x\sim p_t}[u_t^\theta(x)^T u_t^\mathrm{target}(x)]
    &amp;= \iint p_t(x) u_t^\theta(x)^T \left[u_t^\mathrm{target}(x)\right] \, dx \, dt  \\
    &amp;= \iint p_t(x) u_t^\theta(x)^T \int u_t^*(x\mid z) p_t(z\mid x)\, dz \, dx \, dt \\
    &amp;= \iiint u_t^\theta(x)^T u_t^*(x\mid z) p_t(x, z)\, dz\, dx\, dt \\
    &amp;= \mathbb{E}_{t, z\sim p_{\mathrm{data}}, x\sim p_t(\cdot\mid z)} [u_t^\theta(x)^T u_t^*(x\mid z)]
\end{aligned}\]</span>
</div>
<div id="gaussian" class="section level2 unnumbered hasAnchor">
<h2>Gaussian example<a href="flowDiffusion.html#gaussian" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The next order of business is to work out a concrete example; this is almost the only example we’ll ever need to work out. We begin by specifying an almost trivial conditional probability path:</p>
<div class="definition">
<p><span id="def:condGaussianPath" class="definition"><strong>Definition 1.1  (conditional Gaussian probability path) </strong></span>Consider a single point <span class="math inline">\(z\in \mathbb R^d\)</span>, the following <strong>Gaussian probability path</strong> continuously collapses <span class="math inline">\(p_{\mathrm{init}}\)</span> to <span class="math inline">\(\delta_z\)</span>:
<span class="math display">\[
    p_t^\mathrm{target}(x\mid z) = \mathcal N(x; \alpha_t z, \beta_t^2 I_d)
    = \dfrac 1 {(2\pi \beta_t^2)^{d/2}} \exp \left(-\dfrac{\|x-\alpha_t z\|^2}{2\beta_t^2}\right)
\]</span>
where <span class="math inline">\(\alpha_t, \beta_t\)</span> are <strong>noise schedulers</strong> satisfying <span class="math inline">\(\alpha_0=\beta_1=0\)</span> and <span class="math inline">\(\alpha_1=\beta_0=1\)</span>. Note that <span class="math inline">\(x, z\in \mathbb R^d\)</span>. Fixing <span class="math inline">\(z\)</span>, note that <span class="math inline">\(p_t\)</span> is the pushforward measure of <span class="math inline">\(p_{\mathrm{init}}\)</span> under the flow
<span class="math display">\[
    \psi_t^\mathrm{target}(x\mid z) = \alpha_t z + \beta_t x
\]</span></p>
</div>
<div class="proposition">
<p><span id="prp:gaussianCVFS" class="proposition"><strong>Proposition 1.2  (Gaussian conditional vector field and score) </strong></span>Given <span class="math inline">\(\alpha_t, \beta_t\)</span>, the following vector field realizes the conditional Gaussian probability path:
<span class="math display">\[
    u_t^\mathrm{target}(x\mid z) = \left(\dot \alpha_t - \dfrac{\dot \beta_t}{\beta_t} \alpha_t\right) z + \dfrac{\dot \beta_t}{\beta_t} x
\]</span>
For the special linear-scheduler case <span class="math inline">\(\alpha_t=t, \beta_t=1-t\)</span>, we obtain
<span class="math display">\[
    u_t(x\mid z) = \left(1 + \dfrac{t}{1-t}\right) z - \dfrac{1}{1-t} x = \dfrac{z-x} {1-t}
\]</span>
The score function is
<span class="math display">\[
    \nabla \log p_t^\mathrm{target}(x\mid z) = -\dfrac{x - \alpha_t z}{\beta_t^2}
\]</span></p>
</div>
<details>
<summary>
<span style="color:blue">IMPORTANT proof: instead of directly verifying the continuity equation, given <span class="math inline">\(p_t\)</span> construct vector field by (1) constructing a flow <span class="math inline">\(\psi_t(x_0)\)</span> whose pushforward measure of <span class="math inline">\(p_0\)</span> is <span class="math inline">\(p_t\)</span>, and (2) finding <span class="math inline">\(u_t\)</span> such that <span class="math inline">\(\partial_{t} \psi_t(x_0) = u_t(\psi_t(x_0))\)</span></span>
</summary>
Drop the <span class="math inline">\(\mathrm{target}\)</span> superscript to reduce clutter. Instead of doing the hairy calculation for <span class="math inline">\(\partial_{t} p_t\)</span>, we recall that <span class="math inline">\(p_t\)</span> is the pushforward of <span class="math inline">\(p_0=\mathcal N(0, I_d)\)</span> under <span class="math inline">\(\psi_t\)</span>, so it suffices to show that
<span class="math display">\[
    \partial_{t} \psi_t(x\mid z) = u_t(\psi_t(x\mid z)\mid z)
\]</span>
Unpacking this a bit more: fixing <span class="math inline">\(z\)</span> throughout, the map <span class="math inline">\(x\mapsto \psi_t(x\mid z)\)</span> sends initial <span class="math inline">\(x\sim p_{\mathrm{init}}\)</span> to where it would be at time <span class="math inline">\(t\)</span> such that <span class="math inline">\(x_t=\psi_t(x_0\mid z)\sim p_t\)</span> for all <span class="math inline">\(t\)</span>. Also recall that given flow <span class="math inline">\(x_0\mapsto \Phi_t(x_0)\)</span>, the vector field generator of the flow <span class="math inline">\(u_t\)</span> satisfies
<span class="math display">\[
    \partial_{t} \Phi_t(x_0) = \psi_t(x_t) = \psi_t(\Phi_t(x_0))
\]</span>
Returning to our original equation, we have
<span class="math display">\[\begin{aligned}
    u_t(\psi_t(x\mid z)\mid z)
        &amp;= \left(
            \dot \alpha_t - \dfrac{\dot \beta_t}{\beta_t} \alpha_t
        \right) z + \dfrac{\dot \beta_t}{\beta_t} (\alpha_t z + \beta_t x) \\
        &amp;= \dot \alpha_t z + \dot \beta_t x = \partial_{t} \psi_t
\end{aligned}\]</span>
</details>
<ul>
<li>Again, there are many vector fields which can realize the Gaussian conditional probability path. This gauge freedom is embedded in our choice of <span class="math inline">\(\psi_t\)</span>: there are many flow maps which realize the same specified pushforward measure.</li>
</ul>
<div class="proposition">
<p><span id="prp:gaussianFormulae" class="proposition"><strong>Proposition 1.3  (flow and score matching for Gaussian paths) </strong></span></p>
Applying <a href="flowDiffusion.html#prp:gaussianCVFS">1.2</a>, we obtain the losses which make training protocol self-evident:
<span class="math display">\[\begin{aligned}
    \mathcal L_\mathrm{CFM}(\theta)
    &amp;= \mathbb{E}_{t\sim \mathrm{Unif}, z\sim p_{\mathrm{data}}, x\sim \mathcal N(\alpha_tz, \beta_t^2I_d)} \left\|
        u_t^\theta(x) - \left(\dot \alpha(t) - \dfrac{\dot \beta_t}{\beta_t} \alpha_t \right)z
        - \dfrac{\dot \beta_t}{\beta_t} x
    \right\|^2 \\
    &amp;= \mathbb{E}_{t\sim \mathrm{Unif}, z\sim p_{\mathrm{data}}, \epsilon \sim \mathcal N(0, I_d)} \left\|
        u_t^\theta(\alpha_t z + \beta_t \epsilon) + (\dot \alpha_t z + \dot \beta_t \epsilon)
    \right\|
\end{aligned}\]</span>
For simple linear interpolation, this instantiates to <span class="math inline">\(\mathcal L_\mathrm{CFM}(\theta) = \mathbb{E}\|u_t^\theta(tz + \bar t\epsilon) - (z-\epsilon)\|^2\)</span>. In terms of score, we obtain
<span class="math display">\[\begin{aligned}
    \mathcal L_\mathrm{CSM}(\theta)
    &amp;= \mathbb{E}_{t\sim \mathrm{Unif}, z\sim p_{\mathrm{data}}, x\sim p_t(\cdot\mid z)} \left\|
        s_t^\theta(x) + \dfrac{x - \alpha_tz}{\beta_t^2}
    \right\| \\
    &amp;= \mathbb{E}_{t\sim \mathrm{Unif}, z\sim p_{\mathrm{data}}, \epsilon \sim \mathcal N(0, I_d)} \left\|
       s_t^\theta(\alpha_t z + \beta_t\epsilon) + \dfrac{\epsilon}{\beta_t}
    \right\|^2
\end{aligned}\]</span>
<p>The score network essentially tries to learn the noise <span class="math inline">\(\epsilon\)</span> which used to corrupt data. To avoid <span class="math inline">\(\beta\to 0\)</span> instability, Denoising Diffusion Probabilistic Models (DDPM) proposed to drop <span class="math inline">\(1/\beta\)</span> coefficient and predict <span class="math inline">\(\epsilon_t^\theta\)</span>. Some algebra further shows that predicting the score is equivalent to predicting the target vector field:
<span class="math display">\[
    u_t^\mathrm{target}(x\mid z) = \left(\beta_t^2 \dfrac{\dot \alpha_t}{\alpha_t} - \dot \beta_t \beta_t\right) \nabla \log p_t(x\mid z) + \dfrac{\dot \alpha_t}{\alpha_t} x
\]</span>
Since both quantities share marginalization formula, the same equivalence holds for marginal targets.</p>
</div>
</div>
<div id="guidance" class="section level2 unnumbered hasAnchor">
<h2>Guidance<a href="flowDiffusion.html#guidance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We would further like to be able to condition generation on prompts / features / past data <span class="math inline">\(y\in \mathcal Y\)</span>. The generation process becomes
<span class="math display">\[
    Y\xrightarrow{p_{\mathrm{data}}(z\mid y)} Z\xrightarrow{p_t(x\mid z)} X
\]</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-3" class="definition"><strong>Definition 1.2  (guided diffusion model) </strong></span>A guided diffusion (flow) model consists of a guided vector field <span class="math inline">\(u_t^\theta(\cdot \mid y)\)</span>
which accepts <span class="math inline">\(t, y\in \mathcal Y\)</span> to output change in <span class="math inline">\(X\)</span>. Essentially we’re trying to jointly model <span class="math inline">\(p_{\mathrm{data}}(z, y)\)</span>.</p>
</div>
<p>Fixing <span class="math inline">\(y\)</span>, the conditional flow matching loss writes
<span class="math display">\[
    \mathcal L_{\mathrm{CFM}, y}(\theta) = \mathbb{E}_{t\sim \mathrm{Unif}, z\sim p_{\mathrm{data}}(\cdot\mid y), x\sim p_t(\cdot\mid z)} \left\|u_t^\theta(x) - u_t^\mathrm{target}(x\mid z)\right\|^2
\]</span>
Take expectation over <span class="math inline">\(y\)</span> to obtain the guided conditional loss
<span class="math display">\[
    \mathcal L_\mathrm{CFM}^\mathrm{guided}(\theta) = \mathbb{E}_{t\sim \mathrm{Unif}, (z, y)\sim p_{\mathrm{data}}(z, y), x\sim p_t(\cdot\mid z)} \|u_t^\theta(x\mid y) - u_t^\mathrm{target}(x\mid z) \|^2
\]</span>
Empirically, people have observed that the loss above does not enforce guidance strongly enough. Recalling the linear Gaussian formula <a href="flowDiffusion.html#prp:gaussianFormulae">1.3</a>, we can rewrite
<span class="math display">\[
    u_t^\mathrm{target}(x\mid y) = a_t x + b_t \nabla_x \log p_t(x\mid y), \quad (a_t, b_t) = \left(\dfrac{\dot \alpha_t}{\alpha_t}, \dfrac{\dot \alpha_t \beta_t^2 - \dot \beta_t \beta_t \alpha_t}{\alpha_t}\right)
\]</span>
Rewrite guided score as <span class="math inline">\(\nabla \log p_t(x\mid y) = \nabla \log p_t(y\mid x) + \nabla \log p_t(x)\)</span>:
- Guided score at noisily guided data <span class="math inline">\(x_t\)</span> is equivalent to the un-guided score of <span class="math inline">\(x_t\)</span>, plus log-likelihood of the guidance label <span class="math inline">\(y\)</span>, given <span class="math inline">\(x_t\)</span>.</p>
To artificially amplify guidance, consider introducing the <strong>guidance scale</strong> <span class="math inline">\(w&gt;1\)</span> and rearrange
<span class="math display">\[\begin{aligned}
    \tilde u_t(x\mid y)
    &amp;= a_t x + b_t \left(
        \nabla \log p_t(x) + w\, \nabla \log p_t(y\mid x) \right) \\
    &amp;= a_t x + b_t \left(
        \nabla \log p_t(x) + w\, (\nabla \log p_t(x\mid y) - \nabla \log p_t(x)) \right) \\
    &amp;= a_t x + b_t(1-w) \nabla \log p_t(x) + w \nabla \log p_t(x\mid y) \\
    &amp;= w\, u^\mathrm{target}_t(x\mid y) + \bar w\, u^\mathrm{target}_t(x)
\end{aligned}\]</span>
<p>So for Gaussian conditional probability path, <span style="color:blue">up-scaling guidance from classifier = linearly interpolating between <span class="math inline">\(u_t^\mathrm{target}(x\mid y)\)</span> and <span class="math inline">\(u_t^\mathrm{target}(x)\)</span>.$</span>.</p>
<div class="definition">
<p><span id="def:cfg" class="definition"><strong>Definition 1.3  (classifier-free guidance) </strong></span></p>
Per the discussion above, the <strong>classifier-free guided</strong> vector field and scores are defined, given guidance scale <span class="math inline">\(w&gt;1\)</span>, as
<span class="math display">\[\begin{aligned}
    \tilde u_t(x\mid y)
    &amp;= w\, u^\mathrm{target}_t(x\mid y) + \bar w\, u^\mathrm{target}_t(x), \quad
    \tilde s_t(x\mid y)
    = w\, s^\mathrm{target}_t(x\mid y) + \bar w\, s^\mathrm{target}_t(x)
\end{aligned}\]</span>
</div>
<div class="definition">
<p><span id="def:cfgProtocol" class="definition"><strong>Definition 1.4  (training protocol for linear Gaussian path) </strong></span>Given label-data pairs <span class="math inline">\((y_j, z_j)\)</span>, guidance scale <span class="math inline">\(w&gt;1\)</span>, and probability <span class="math inline">\(\eta\in (0, 1)\)</span>, the training procedure for CFG-flow matching with linear Gaussian probability path <a href="flowDiffusion.html#def:condGaussianPath">1.1</a> is as follows:</p>
<ol style="list-style-type: decimal">
<li>Sample a batch <span class="math inline">\((y_j, z_j)\)</span> from dataset.</li>
<li>Given the batch, sample a boolean mask <span class="math inline">\((\xi_1, \dots, \xi_n)\)</span> with probability <span class="math inline">\(\eta\)</span> denoting whether we’re ignoring the guidance label on the sample. Replace <span class="math inline">\(y_j\mapsto \xi_j \emptyset + \bar \xi_j y_j\)</span>.</li>
<li>Sample <span class="math inline">\(t\sim \mathrm{Unif}, \epsilon \sim \mathcal N(0, I_d)\)</span> batch-wise.</li>
<li>Minimize flow matching loss:
<span class="math display">\[\begin{aligned}
\mathcal L^{\mathrm{CFG}}_\mathrm{CSM}
= \dfrac 1 n \sum_{j=1}^n \left\|
     u_{t_j}^\theta(t_jz_j + \bar t_j\epsilon_j\mid y_j) - (z_j-\epsilon_j)
\right\|^2
\end{aligned}\]</span></li>
</ol>
<p>This trains the model to match flow of <span class="math inline">\(u_t^\mathrm{target}(x\mid y)\)</span> for conditional case, and <span class="math inline">\(u_t^\mathrm{target}(x\mid \emptyset)\)</span> for unconditional case. Guided sampling given <span class="math inline">\(y\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Initialize <span class="math inline">\(x_0\sim p_{\mathrm{init}}\)</span>.</li>
<li>Simulate ODE <span class="math inline">\(dx_t = \left[ w\, u^\theta_t(x_t\mid y) + \bar w\, u^\theta_t(x_t\mid \emptyset) \right]\, dt\)</span></li>
<li>Output <span class="math inline">\(x_1\)</span>.</li>
</ol>
</div>
<p>Empirically, we want at least O(1) unconditional samples per batch on average. The choice <span class="math inline">\(\eta=0.1\)</span> is common.</p>

</div>
</div>
<h3>Bibliography<a href="bibliography.html#bibliography" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-holderrieth2025introduction" class="csl-entry">
Holderrieth, Peter, and Ezra Erives. 2025. <span>“Introduction to Flow Matching and Diffusion Models.”</span><br />
url<span>https://diffusion.csail.mit.edu/docs/lecture-notes.pdf</span>.
<div id="ref-li2025unified" class="csl-entry">
Li, Marvin. 2025. <span>“In the Blink of an Eye: A Unified Theory for Feature Emergence in Generative Models.”</span> Honors thesis, Cambridge, MA: Harvard College. <a href="https://marvinfli.github.io/files/thesis.pdf">https://marvinfli.github.io/files/thesis.pdf</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliography.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
